{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an RCNN model on a custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image, ExifTags\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Creating Custom Dataset Class\n",
    "The reference scripts for training object detection, instance segmentation and person keypoint detection allows for easily supporting adding new custom datasets. The dataset should inherit from the standard torch.utils.data.Dataset class, and implement `__len__` and `__getitem__`.\n",
    "\n",
    "The only specificity that we require is that the dataset `__getitem__` should return a tuple:\n",
    "\n",
    "- image:   `torchvision.tv_tensors.Image` of shape `[3, H, W]`, a pure tensor, or a PIL Image of size `(H, W)`\n",
    "\n",
    "\n",
    "- target: a dict containing the following fields\n",
    "\n",
    "\n",
    "    * `boxes`, `torchvision.tv_tensors.BoundingBoxes` of shape `[N, 4]`: the coordinates of the `N` bounding boxes in `[x0, y0, x1, y1]` format, ranging from `0` to `W` and `0` to `H`\n",
    "\n",
    "\n",
    "    * `labels`, `integer torch.Tensor` of shape `[N]`: the label for each bounding box. `0` represents always the background class.  \n",
    "\n",
    "\n",
    "    * `image_id`, int: an image identifier. It should be unique between all the images in the dataset, and is used during evaluation  \n",
    "\n",
    "\n",
    "    * `area`, float `torch.Tensor` of shape `[N]`: the area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.\n",
    "\n",
    "\n",
    "    * `iscrowd`, uint8 `torch.Tensor` of shape `[N]`: instances with iscrowd=True will be ignored during evaluation.\n",
    "\n",
    "    \n",
    "    * (optionally) `masks`, `torchvision.tv_tensors.Mask` of shape `[N, H, W]`: the segmentation masks for each one of the objects\n",
    "\n",
    "[source](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)\n",
    "\n",
    "\n",
    "### Errors Encountered:\n",
    "`AssertionError: Results do not correspond to current coco set - wrong types and sizes`\n",
    "\n",
    "Solution: Just change the line `target[\"image_id\"] = torch.tensor([index])` to `target[\"image_id\"] = index`\n",
    "\n",
    "[source](https://stackoverflow.com/questions/76798069/assertionerror-results-do-not-correspond-to-current-coco-set-wrong-types-and)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory containing the 'power_bank' and 'spec_case' folders.\n",
    "            transforms (callable, optional): A function/transform to apply to the images.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "        self.image_paths = []\n",
    "        self.annotation_paths = []\n",
    "        self.box_dims = []  # Initialize box_dims to store widths and heights of bounding boxes\n",
    "        \n",
    "        # Collect all images and their corresponding XML files\n",
    "        for class_dir in os.listdir(root_dir):\n",
    "            class_path = os.path.join(root_dir, class_dir)\n",
    "            if os.path.isdir(class_path):\n",
    "                for file in os.listdir(class_path):\n",
    "                    if file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
    "                        self.image_paths.append(os.path.join(class_path, file))\n",
    "                        self.annotation_paths.append(\n",
    "                            os.path.join(class_path, file.replace(\".jpg\", \".xml\").replace(\".png\", \".xml\"))\n",
    "                        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "\n",
    "    def parse_annotation(self, annotation_path):\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        area = []\n",
    "        iscrowd = []\n",
    "\n",
    "        #Fiinding resolution for rotation\n",
    "        res = root.find(\"size\")\n",
    "        xres = int(res.find(\"width\").text)\n",
    "        yres = int(res.find(\"height\").text)\n",
    "        \n",
    "        for obj in root.findall(\"object\"):\n",
    "            name = obj.find(\"name\").text\n",
    "            label = 1 if name == \"power_bank\" else 2  # Assign class labels\n",
    "            labels.append(label)\n",
    "            \n",
    "            bbox = obj.find(\"bndbox\")\n",
    "            xmin = int(bbox.find(\"xmin\").text)\n",
    "            ymin = int(bbox.find(\"ymin\").text)\n",
    "            xmax = int(bbox.find(\"xmax\").text)\n",
    "            ymax = int(bbox.find(\"ymax\").text)\n",
    "\n",
    "            #Testing\n",
    "            #xmin, ymin, xmax, ymax = ymin, xmax, ymax, xmin\n",
    "            #ymin, ymax = ymax, ymin\n",
    "            \n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            #Testing, Delete after testing\n",
    "            # print('-'*5, \"Inside Class\", '-'*5)\n",
    "            # print(\"xmin = \", xmin)\n",
    "            # print(\"ymin = \", ymin)\n",
    "            # print(\"xmax = \", xmax)\n",
    "            # print(\"ymax = \", ymax)\n",
    "            # print(annotation_path)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            area.append((xmax - xmin) * (ymax - ymin))\n",
    "            iscrowd.append(0)  # Assuming no crowd instances\n",
    "\n",
    "        return {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "            \"area\": torch.tensor(area, dtype=torch.float32),\n",
    "            \"iscrowd\": torch.tensor(iscrowd, dtype=torch.uint8),\n",
    "            \"img_resolution\" : (xres, yres)\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.image_paths[idx]\n",
    "        annotation_path = self.annotation_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        #Testing\n",
    "        #img = img.rotate(270, expand=True)\n",
    "        #img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        # Parse annotation\n",
    "        target = self.parse_annotation(annotation_path)\n",
    "        # target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"image_id\"] = idx\n",
    "\n",
    "        img_res = target.pop(\"img_resolution\")\n",
    "\n",
    "        \"\"\"\n",
    "        If the image is :\n",
    "        \n",
    "        <width>1920</width>\n",
    "\t\t<height>1440</height>\n",
    "        then no rotation.\n",
    "\n",
    "        But if the image is:\n",
    "\n",
    "        <width>1440</width>\n",
    "\t\t<height>1920</height>\n",
    "        then rotate 270 degrees \n",
    "        \"\"\"     \n",
    "        if img_res == (1440, 1920):\n",
    "            img = img.rotate(270, expand=True)\n",
    "\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Testing the dataset\n",
    "\n",
    "This section is entirely optional. We just print some images and their corresponding bounding boxes using our custom dataset class. Remember to correctly set the directory of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def visualize_dataset(dataset, num_images=5):\n",
    "    \"\"\"\n",
    "    Visualize a few images from the dataset with their bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The dataset object.\n",
    "        num_images (int): Number of images to visualize.\n",
    "    \"\"\"\n",
    "    for i in range(num_images):\n",
    "        # Get image and target from the dataset\n",
    "        img, target = dataset[150 + i]\n",
    "        \n",
    "        # Convert image tensor to PIL Image if needed\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = to_pil_image(img)\n",
    "        \n",
    "        # Create a Matplotlib figure\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "        ax.imshow(img, origin=\"upper\")\n",
    "        \n",
    "        # Add bounding boxes to the image\n",
    "        boxes = target[\"boxes\"]\n",
    "        labels = target[\"labels\"]\n",
    "        for box, label in zip(boxes, labels):\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            width = xmax - xmin\n",
    "            height = ymax - ymin\n",
    "            rect = patches.Rectangle(\n",
    "                (xmin, ymin), width, height,\n",
    "                linewidth=2, edgecolor='r', facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(xmin, ymin - 5, f\"Class {label.item()}\", \n",
    "                    color='red', fontsize=12, backgroundcolor='white')\n",
    "        \n",
    "        #ax.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "dataset = CustomDataset(root_dir=\"/home/adarshh/For Github/RCNN/custom_sample_dataset\")\n",
    "visualize_dataset(dataset, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Finetuning from a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# load a model pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 3  # 2 class (spectacle case and power bank) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "def get_model_faster_rcnn(num_classes):\n",
    "    # Load a pre-trained Faster R-CNN model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "    # Get the number of input features for the box classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    # Replace the box predictor with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Installing helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Helper functions for data augmentation / transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        #transforms.append(v2.RandomHorizontalFlip(0.5))\n",
    "        pass    #Randomly flipping throws of the bounding box locations\n",
    "    # transforms.append(v2.ToDtype(torch.float, scale=True))\n",
    "    # transforms.append(v2.ToPureTensor())\n",
    "    transforms.append(T.ToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.float))\n",
    "    # transforms.append(T.ToPILImage())  # Converts PIL to Tensor (New alternative to ToTensor())\n",
    "    # transforms.append(T.ToDtype(torch.float, scale=True))   # Ensures dtype conversion\n",
    "\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Testing `forward()` method (Optional)\n",
    "\n",
    "Before iterating over the dataset, itâ€™s good to see what the model expects during training and inference time on sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from torchvision import transforms\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights = \"DEFAULT\")\n",
    "\n",
    "dataset = CustomDataset('/home/adarshh/For Github/RCNN/custom_sample_dataset', get_transform(train=True))\n",
    "\n",
    "#Only for debugging\n",
    "# img, target = dataset[0]\n",
    "# print(type(img))\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "# For Training\n",
    "images, targets = next(iter(data_loader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]   #Shallow Copying: The operation ensures each target dictionary is reconstructed, preventing unintended side effects when modifying targets.\n",
    "output = model(images, targets)  # Returns losses and detections\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Main function that performs the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has three classes - power_bank, spec_case and background\n",
    "num_classes = 3\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "dataset = CustomDataset('/home/adarshh/For Github/RCNN/custom_sample_dataset', get_transform(train=True))\n",
    "dataset_test = CustomDataset('/home/adarshh/For Github/RCNN/custom_sample_dataset', get_transform(train=False))\n",
    "\n",
    "\"\"\"Stratified Split\n",
    "\n",
    "We need to ensure that both classes appear in the training and test sets. Since you have:\n",
    "\n",
    "    131 power bank images\n",
    "    130 spec case images\n",
    "\n",
    "We should keep the 80:20 ratio within each class:\n",
    "\n",
    "    Power bank: 131x0.8=104 (train), 131x0.2=27 (test)\n",
    "    Spec case: 130x0.8=104 (train), 130x0.2=26 (test)\"\"\"\n",
    "\n",
    "# Separate indices for power bank and spec case images\n",
    "power_bank_indices = list(range(131))  # First 131 are power banks\n",
    "spec_case_indices = list(range(131, 261))  # Next 130 are spec cases\n",
    "\n",
    "# Shuffle the indices\n",
    "import random\n",
    "random.shuffle(power_bank_indices)\n",
    "random.shuffle(spec_case_indices)\n",
    "\n",
    "# 80% train, 20% test split\n",
    "train_indices = power_bank_indices[:104] + spec_case_indices[:104]\n",
    "test_indices = power_bank_indices[104:] + spec_case_indices[104:]\n",
    "\n",
    "# Create subsets\n",
    "dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, test_indices)\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_faster_rcnn(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "# let's train it just for 2 epochs\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "print(\"That's it!\")\n",
    "\n",
    "best_model_params_path = '/home/adarshh/For Github/RCNN/best_model.pt'\n",
    "torch.save(model.state_dict(), best_model_params_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Testing on Droidcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading already trained model\n",
    "num_classes = 3\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=False, num_classes=num_classes)\n",
    "best_model_params_path = '/home/adarshh/For Github/RCNN/best_model.pt'\n",
    "model.load_state_dict(torch.load(best_model_params_path, weights_only=True))\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)  # Move model to device\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import threading\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "def capture_frames(cap):\n",
    "    global frame\n",
    "    while True:\n",
    "        ret, new_frame = cap.read()\n",
    "        if ret:\n",
    "            frame = new_frame  # Always update to the latest frame\n",
    "\n",
    "def live_object_detection(model, device, droidcam_url, score_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Captures live feed from DroidCam and performs real-time object detection with FPS display.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained Faster R-CNN model.\n",
    "        device (torch.device): CUDA or CPU device.\n",
    "        droidcam_url (str): The URL of the DroidCam feed (e.g., \"http://192.168.1.2:4747/video\").\n",
    "        score_threshold (float): Minimum confidence score to display a bounding box.\n",
    "    \"\"\"\n",
    "\n",
    "    global frame\n",
    "    cap = cv2.VideoCapture(droidcam_url)\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # Reduce buffer size\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open DroidCam feed. Check the URL.\")\n",
    "        return\n",
    "    \n",
    "    # Start a separate thread for frame capture\n",
    "    threading.Thread(target=capture_frames, args=(cap,), daemon=True).start()\n",
    "    \n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    prev_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        if frame is None:\n",
    "            continue  # Wait until frame is available\n",
    "        \n",
    "        # Calculate FPS\n",
    "        curr_time = time.time()\n",
    "        fps = 1 / (curr_time - prev_time)\n",
    "        prev_time = curr_time\n",
    "\n",
    "        # Convert frame from BGR (OpenCV) to RGB (for PyTorch)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img_tensor = F.to_tensor(rgb_frame).unsqueeze(0).to(device)\n",
    "\n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            prediction = model(img_tensor)[0]\n",
    "\n",
    "        # Extract boxes, labels, and scores\n",
    "        pred_boxes = prediction[\"boxes\"].cpu().numpy()\n",
    "        pred_labels = prediction[\"labels\"].cpu().numpy()\n",
    "        pred_scores = prediction[\"scores\"].cpu().numpy()\n",
    "\n",
    "        # Draw bounding boxes on the frame\n",
    "        for box, label, score in zip(pred_boxes, pred_labels, pred_scores):\n",
    "            if score >= score_threshold:\n",
    "                xmin, ymin, xmax, ymax = map(int, box)\n",
    "                cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"Class {label} ({score:.2f})\", \n",
    "                            (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                            0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Display FPS on the frame\n",
    "        cv2.putText(frame, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        # Display the frame with bounding boxes\n",
    "        cv2.imshow(\"Live Object Detection\", frame)\n",
    "\n",
    "        # Break on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "droidcam_url = \"http://XXX.XXX.X.XX:4747/video\"     # Change this to your DroidCam IP\n",
    "frame = None  # Initialize global frame variable\n",
    "live_object_detection(model, device, droidcam_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
